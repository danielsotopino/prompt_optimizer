# Self-Supervised Prompt Optimization (SPO) Framework

Una implementaci√≥n completa del framework SPO basado en MetaGPT para optimizaci√≥n automatizada de prompts usando OpenAI. Este proyecto est√° inspirado en las t√©cnicas de optimizaci√≥n de prompts documentadas por Mistral AI y adaptado para trabajar con modelos de OpenAI.

## üöÄ Caracter√≠sticas

- **Optimizaci√≥n Iterativa**: Mejora autom√°tica de prompts a trav√©s de m√∫ltiples iteraciones
- **M√∫ltiples Estrategias**: Soporte para diferentes enfoques de optimizaci√≥n
- **Sistema de Evaluaci√≥n Integral**: M√©tricas comprehensivas para evaluar calidad de prompts
- **Pipeline Avanzado**: Comparaci√≥n de estrategias y optimizaci√≥n multi-objetivo
- **Funciones Lambda MetaGPT**: Implementaci√≥n fiel al notebook original con funciones lambda
- **Puntuaci√≥n de Confianza**: Sistema avanzado para medir confiabilidad de resultados
- **Criterios de Parada Autom√°tica**: Detecci√≥n inteligente de convergencia
- **Soporte YAML/JSON**: Formatos flexibles para definir prompts e inputs
- **Ejemplo Pr√°ctico**: Demostraci√≥n completa con clasificaci√≥n de t√≠tulos de trabajo

## üìÅ Estructura del Proyecto

```
prompt_optimizer/
‚îú‚îÄ‚îÄ spo_framework.py          # Framework principal SPO
‚îú‚îÄ‚îÄ optimization_pipeline.py  # Pipeline avanzado con m√∫ltiples estrategias
‚îú‚îÄ‚îÄ evaluation_system.py      # Sistema de evaluaci√≥n comprehensivo
‚îú‚îÄ‚îÄ lambda_functions.py       # Funciones lambda estilo MetaGPT
‚îú‚îÄ‚îÄ confidence_scoring.py     # Sistema de puntuaci√≥n de confianza
‚îú‚îÄ‚îÄ yaml_parser.py            # Parser para archivos YAML/JSON
‚îú‚îÄ‚îÄ main.py                   # Interfaz CLI
‚îú‚îÄ‚îÄ requirements.txt          # Dependencias (incluye PyYAML)
‚îú‚îÄ‚îÄ .env                      # Variables de entorno (API keys)
‚îú‚îÄ‚îÄ .gitignore                # Archivos a ignorar en git
‚îú‚îÄ‚îÄ examples/                 # Ejemplos de configuraci√≥n (‚úÖ Probados)
‚îÇ   ‚îú‚îÄ‚îÄ sample_inputs.json        # Ejemplo b√°sico en JSON
‚îÇ   ‚îú‚îÄ‚îÄ sample_inputs.yaml        # Ejemplo b√°sico en YAML
‚îÇ   ‚îú‚îÄ‚îÄ sample_expected.json      # Salidas esperadas para los ejemplos
‚îÇ   ‚îú‚îÄ‚îÄ classification_example.yaml # Ejemplo completo de clasificaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ prompt_config_example.yaml  # Configuraci√≥n con prompt incluido
‚îÇ   ‚îî‚îÄ‚îÄ globot.yaml               # Ejemplo avanzado de clasificaci√≥n de mensajes
‚îî‚îÄ‚îÄ README.md                # Este archivo
```

## üõ†Ô∏è Instalaci√≥n

1. **Crear entorno virtual Python 3.11**:
```bash
python3.11 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

2. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

3. **Configurar API Key de OpenAI**:
```bash
export OPENAI_API_KEY="tu-api-key-aqui"
```

## üéØ Uso R√°pido

### üöÄ Prueba R√°pida del Framework (Recomendado para empezar)

```bash
# 1. Activar entorno virtual
source venv/bin/activate

# 2. Configurar API key de OpenAI
export OPENAI_API_KEY="tu-api-key-aqui"

# 3. Ejecutar prueba r√°pida (2-3 minutos, ~$0.10-0.20)
python test_optimization.py
```

**Lo que hace la prueba r√°pida:**
- ‚ö° 3 iteraciones de optimizaci√≥n b√°sica
- üéØ Caso de uso: clasificaci√≥n de t√≠tulos de trabajo
- üìä Muestra puntuaciones y mejoras en tiempo real
- üí∞ Usa `gpt-4o-mini` para menor costo
- ‚è±Ô∏è Completa en 2-3 minutos

### Ejemplo de Clasificaci√≥n de T√≠tulos de Trabajo

```bash
# Demo r√°pido (5-10 minutos, ~$0.50-1.00)
python main.py job-title-example --mode quick --output quick_results.json

# Demo completo con todas las caracter√≠sticas (15-20 minutos, ~$2.00-4.00)
python main.py job-title-example --mode full --output full_results.json
```

### üéõÔ∏è Opciones de Prueba Avanzadas

#### Optimizaci√≥n Personalizada

```bash
# Usando archivos JSON tradicionales
python main.py optimize \
  --prompt "Clasifica el t√≠tulo de trabajo en categor√≠as. Formato: Categor√≠a - Subcategor√≠a. T√≠tulo: {job_title}" \
  --task "Clasificaci√≥n de t√≠tulos de trabajo" \
  --inputs examples/sample_inputs.json \
  --expected examples/sample_expected.json \
  --iterations 5 \
  --output optimization_results.json

# Usando archivos YAML (nuevo)
python main.py optimize \
  --prompt "Clasifica el t√≠tulo de trabajo en categor√≠as" \
  --task "Clasificaci√≥n de t√≠tulos de trabajo" \
  --inputs examples/sample_inputs.yaml \
  --expected examples/sample_expected.yaml \
  --iterations 5 \
  --output optimization_results.json

# Usando archivo YAML completo con prompt incluido
python main.py optimize \
  --inputs examples/classification_example.yaml \
  --output optimization_results.json

# Ejemplo avanzado con clasificaci√≥n de mensajes de chat
python main.py optimize \
  --inputs examples/globot.yaml \
  --output globot_optimization_results.json
```

#### Comparaci√≥n de Estrategias
```bash
# Con archivos JSON
python main.py compare \
  --prompt "Clasifica el t√≠tulo de trabajo en categor√≠as apropiadas" \
  --task "Clasificaci√≥n de t√≠tulos de trabajo" \
  --inputs examples/sample_inputs.json \
  --strategies iterative_refinement ensemble_voting multi_objective \
  --output strategy_comparison.json

# Con archivos YAML
python main.py compare \
  --inputs examples/classification_example.yaml \
  --strategies iterative_refinement ensemble_voting \
  --output strategy_comparison.json
```

#### Solo Evaluaci√≥n (sin optimizaci√≥n)
```bash
# Con archivos JSON
python main.py evaluate \
  --prompt "Tu prompt a evaluar" \
  --inputs examples/sample_inputs.json \
  --outputs examples/sample_outputs.json \
  --expected examples/sample_expected.json \
  --metrics accuracy relevance clarity \
  --output evaluation_results.json

# Con archivos YAML
python main.py evaluate \
  --prompt "Tu prompt a evaluar" \
  --inputs examples/classification_example.yaml \
  --outputs examples/outputs.yaml \
  --expected examples/classification_example.yaml \
  --metrics accuracy relevance clarity \
  --output evaluation_results.json
```

## üìù Soporte para Formatos YAML y JSON

### Formato YAML Completo (Recomendado)

El framework ahora soporta archivos YAML que incluyen tanto el prompt como los datos de entrada y salida esperada en un solo archivo:

```yaml
prompt: |
  Eres un agente especializado en clasificar mensajes de usuarios.
  Tu tarea es analizar cada mensaje y clasificarlo en una de las siguientes categor√≠as:
  - business_relevant: Mensajes relacionados con consultas de negocio o productos
  - support_request: Solicitudes de ayuda o soporte t√©cnico
  - complaint: Quejas o expresiones de insatisfacci√≥n
  - compliment: Elogios o comentarios positivos
  
  Responde √∫nicamente con la categor√≠a correspondiente.

task: "Clasificar mensajes de usuarios en categor√≠as predefinidas"

data:
  - message: "Hola, ¬øtienen stock de computadores?"
    classification: "business_relevant"
  - message: "Mi cuenta no funciona, necesito ayuda"
    classification: "support_request"
  - message: "Estoy muy molesto porque mi pedido lleg√≥ tarde"
    classification: "complaint"
  - message: "Excelente servicio, muy recomendado"
    classification: "compliment"

iterations: 5
model: "gpt-4o"
```

### Formatos Soportados

1. **YAML con datos combinados** (como el ejemplo anterior)
2. **YAML con secciones separadas**:
```yaml
inputs:
  - "Mensaje 1"
  - "Mensaje 2"
expected:
  - "Categor√≠a 1"
  - "Categor√≠a 2"
```

3. **JSON tradicional** (mantiene compatibilidad):
```json
["input1", "input2", "input3"]
```

### Uso del Parser YAML

```python
from yaml_parser import YAMLParser

# Cargar inputs desde cualquier formato
inputs = YAMLParser.load_inputs_flexible("examples/archivo.yaml")

# Cargar outputs esperados
expected = YAMLParser.load_expected_flexible("examples/archivo.yaml")

# Cargar configuraci√≥n completa
config = YAMLParser.parse_prompt_config("examples/classification_example.yaml")
```

## üìÇ Directorio de Ejemplos

El directorio `examples/` contiene archivos de ejemplo para diferentes casos de uso:

### üìÑ Archivos Disponibles

1. **`sample_inputs.json`** - Ejemplo b√°sico en formato JSON
   ```json
   [
     "Senior Software Engineer at Google",
     "Data Scientist - Machine Learning",
     "Marketing Manager, Digital Products"
   ]
   ```

2. **`sample_inputs.yaml`** - Mismo contenido en formato YAML
   ```yaml
   inputs:
     - "Senior Software Engineer at Google"
     - "Data Scientist - Machine Learning"
     - "Marketing Manager, Digital Products"
   ```

3. **`classification_example.yaml`** - Ejemplo completo con prompt y datos
   ```yaml
   prompt: |
     Clasifica mensajes en categor√≠as espec√≠ficas...
   
   data:
     - message: "Hola, ¬øtienen stock?"
       classification: "business_relevant"
   ```

4. **`globot.yaml`** - Ejemplo avanzado de clasificaci√≥n de mensajes de chat
   - Prompt detallado con reglas espec√≠ficas
   - 20+ ejemplos de entrenamiento
   - 3 categor√≠as: business_relevant, contact_message, chitchat

5. **`prompt_config_example.yaml`** - Configuraci√≥n con m√∫ltiples par√°metros
   ```yaml
   prompt: |
     Tu prompt aqu√≠...
   task: "Descripci√≥n de la tarea"
   iterations: 5
   model: "gpt-4o"
   strategies: ["iterative_refinement"]
   ```

### üöÄ C√≥mo Usar los Ejemplos (‚úÖ Todos Probados)

```bash
# 1. Ejemplo b√°sico con prompt externo
python main.py optimize \
  --inputs examples/sample_inputs.yaml \
  --prompt "Classify job titles into categories" \
  --task "Job title classification" \
  --iterations 2 \
  --output results.yaml

# 2. Ejemplo completo con prompt incluido
python main.py optimize \
  --inputs examples/classification_example.yaml \
  --iterations 3 \
  --output classification_results.yaml

# 3. Ejemplo avanzado de clasificaci√≥n de mensajes (globot)
python main.py optimize \
  --inputs examples/globot.yaml \
  --iterations 3 \
  --output globot_results.yaml

# 4. Usando archivos JSON tradicionales
python main.py optimize \
  --prompt "Classify these job titles" \
  --task "Job classification" \
  --inputs examples/sample_inputs.json \
  --expected examples/sample_expected.json \
  --output traditional_results.json
```

### ‚úÖ Resultados de Pruebas

Los ejemplos han sido probados y funcionan correctamente:

- **`classification_example.yaml`**: ‚úÖ Score perfecto (1.00) en 1 iteraci√≥n
- **`prompt_config_example.yaml`**: ‚úÖ Score perfecto (1.00) en 1 iteraci√≥n
- **`sample_inputs.yaml`**: ‚úÖ Score mejorado (0.68) en 2 iteraciones  
- **`globot.yaml`**: ‚úÖ Score excelente (0.91+) demostrado anteriormente
- **Salida YAML**: ‚úÖ Formato legible y bien estructurado
- **Todos los formatos**: ‚úÖ JSON y YAML funcionando correctamente

### Ventajas del Formato YAML

- **Prompts multil√≠nea**: Usa `|` para prompts largos y complejos
- **Un solo archivo**: Todo en un lugar (prompt, datos, configuraci√≥n)
- **M√°s legible**: Formato m√°s claro que JSON
- **Comentarios**: Puedes agregar comentarios con `#`
- **Flexible**: M√∫ltiples formatos soportados
- **Salida en YAML**: Los resultados tambi√©n pueden guardarse en YAML

### üíæ Formatos de Salida

El framework detecta autom√°ticamente el formato de salida basado en la extensi√≥n del archivo:

```bash
# Salida en JSON (tradicional)
python main.py optimize --inputs examples/globot.yaml --output results.json

# Salida en YAML (nuevo, m√°s legible)
python main.py optimize --inputs examples/globot.yaml --output results.yaml
```

**Ejemplo de salida en YAML:**
```yaml
config:
  max_iterations: 5
  optimization_model: "gpt-4o"
  execution_model: "gpt-4o-mini"

optimization_history:
  - iteration: 1
    performance_score: 0.91
    optimized_prompt: "Prompt optimizado..."
    execution_time: 45.2

summary:
  total_iterations: 5
  best_score: 0.95
  improvement: 0.15
  best_prompt: "Mejor prompt encontrado..."
```

## üìä Estrategias de Optimizaci√≥n

### 1. Refinamiento Iterativo (`iterative_refinement`)
- **Mejor para**: Optimizaci√≥n de prop√≥sito general
- **Descripci√≥n**: Mejora incremental paso a paso
- **Ventajas**: R√°pido y eficiente

### 2. Votaci√≥n de Ensemble (`ensemble_voting`)
- **Mejor para**: Resultados robustos y confiables
- **Descripci√≥n**: M√∫ltiples optimizaciones en paralelo
- **Ventajas**: Mayor estabilidad y precisi√≥n

### 3. Multi-Objetivo (`multi_objective`)
- **Mejor para**: Balance entre objetivos competitivos
- **Descripci√≥n**: Optimiza para m√∫ltiples criterios simult√°neamente
- **Ventajas**: Soluciones balanceadas

### 4. Algoritmo Gen√©tico (`genetic_algorithm`)
- **Mejor para**: Exploraci√≥n creativa y diversa
- **Descripci√≥n**: Evoluci√≥n de prompts atrav√©s de generaciones
- **Ventajas**: Encuentra soluciones innovadoras

## üîß Uso Program√°tico

### Optimizaci√≥n B√°sica

```python
import asyncio
from spo_framework import SPOFramework, PromptOptimizationConfig

async def optimize_prompt():
    config = PromptOptimizationConfig(
        max_iterations=5,
        optimization_model="gpt-4o",
        execution_model="gpt-4o-mini"
    )
    
    framework = SPOFramework(config, "tu-api-key")
    
    result = await framework.optimize_prompt(
        initial_prompt="Tu prompt inicial",
        task_description="Descripci√≥n de la tarea",
        sample_inputs=["input1", "input2", "input3"],
        expected_outputs=["output1", "output2", "output3"]
    )
    
    print(f"Score: {result.performance_score}")
    print(f"Prompt optimizado: {result.optimized_prompt}")

asyncio.run(optimize_prompt())
```

### Optimizaci√≥n Estilo MetaGPT con Funciones Lambda

```python
from lambda_functions import MetaGPTStyleOptimizer, PromptLambdaFactory

async def metagpt_optimization():
    # Crear funci√≥n lambda para clasificaci√≥n de trabajos
    job_classifier = PromptLambdaFactory.create_job_classification_lambda()
    
    # Optimizador estilo MetaGPT
    optimizer = MetaGPTStyleOptimizer(client, config)
    
    result = await optimizer.optimize_with_lambda(
        initial_lambda=job_classifier,
        task_description="Clasificar t√≠tulos de trabajo",
        test_inputs=job_titles,
        expected_outputs=expected_classifications,
        max_rounds=5
    )
```

### Pipeline Avanzado

```python
from optimization_pipeline import AdvancedOptimizationPipeline, PipelineConfig, OptimizationStrategy

async def advanced_optimization():
    pipeline_config = PipelineConfig(
        strategy=OptimizationStrategy.ENSEMBLE_VOTING,
        ensemble_size=5
    )
    
    pipeline = AdvancedOptimizationPipeline(spo_config, pipeline_config, api_key)
    
    result = await pipeline.optimize_with_strategy(
        initial_prompt="Tu prompt",
        task_description="Tarea",
        sample_inputs=inputs,
        expected_outputs=outputs
    )
```

### Sistema de Confianza

```python
from confidence_scoring import ConfidenceAnalyzer

async def analyze_confidence():
    analyzer = ConfidenceAnalyzer(client)
    
    confidence_metrics = await analyzer.analyze_optimization_confidence(
        optimization_history=optimization_results,
        test_inputs=test_data,
        final_prompt=best_prompt,
        num_validation_runs=5
    )
    
    report = analyzer.generate_confidence_report(confidence_metrics)
    print(f"Confidence Level: {report['confidence_level']}")
    print(f"Reliability Index: {report['reliability_index']:.2f}")
```

### Evaluaci√≥n Comprehensiva

```python
from evaluation_system import ComprehensiveEvaluationSystem, EvaluationCriteria, EvaluationMetric

async def evaluate_prompt():
    evaluation_system = ComprehensiveEvaluationSystem(api_key)
    
    criteria = [
        EvaluationCriteria(EvaluationMetric.ACCURACY, 0.5, "Precisi√≥n"),
        EvaluationCriteria(EvaluationMetric.CLARITY, 0.3, "Claridad"),
        EvaluationCriteria(EvaluationMetric.RELEVANCE, 0.2, "Relevancia")
    ]
    
    result = await evaluation_system.comprehensive_evaluate(
        prompt="Tu prompt",
        input_text="Input de prueba",
        output="Output generado",
        expected="Output esperado",
        evaluation_criteria=criteria
    )
```

## üìà M√©tricas de Evaluaci√≥n

- **Accuracy**: Precisi√≥n y correctitud del output
- **Relevance**: Relevancia al input y tarea
- **Clarity**: Claridad y legibilidad
- **Completeness**: Completitud de la respuesta
- **Consistency**: Consistencia en m√∫ltiples ejecuciones
- **Efficiency**: Eficiencia del prompt
- **Adherence to Format**: Adherencia al formato especificado
- **Confidence Metrics**: M√©tricas de confiabilidad y estabilidad

## üîç Monitoreo y Resultados

Los resultados se pueden exportar en formato JSON para an√°lisis posterior:

```json
{
  "config": {...},
  "optimization_history": [...],
  "summary": {
    "total_iterations": 5,
    "best_score": 0.89,
    "improvement": 0.34,
    "best_prompt": "Prompt optimizado final"
  }
}
```

## üéØ Ejemplo Detallado: Clasificaci√≥n de T√≠tulos de Trabajo

El framework incluye un ejemplo completo que demuestra la optimizaci√≥n de un sistema de clasificaci√≥n de t√≠tulos de trabajo:

```python
from job_title_example import JobTitleClassificationExample

async def run_example():
    example = JobTitleClassificationExample(api_key)
    results = await example.run_complete_example()
    
    # Ejecuta:
    # 1. Optimizaci√≥n b√°sica SPO
    # 2. Comparaci√≥n de estrategias
    # 3. Evaluaci√≥n comprehensiva
    # 4. Reporte final con recomendaciones
```

## üöÄ Casos de Uso

- **Mejora de Chatbots**: Optimizar prompts para conversaciones m√°s naturales
- **Clasificaci√≥n de Texto**: Mejorar precisi√≥n en tareas de categorizaci√≥n
- **Generaci√≥n de Contenido**: Optimizar prompts para contenido de calidad
- **An√°lisis de Sentimientos**: Afinar prompts para mejor detecci√≥n emocional
- **Extracci√≥n de Informaci√≥n**: Mejorar prompts para extraer datos estructurados

## üîß Configuraci√≥n Avanzada

### Modelos Personalizados

```python
config = PromptOptimizationConfig(
    optimization_model="gpt-4o",        # Modelo para optimizaci√≥n
    execution_model="gpt-4o-mini",      # Modelo para ejecuci√≥n
    evaluation_model="gpt-4o",          # Modelo para evaluaci√≥n
    temperature=0.7,                     # Creatividad
    max_tokens=2000                      # L√≠mite de tokens
)
```

### Criterios de Evaluaci√≥n Personalizados

```python
custom_criteria = [
    EvaluationCriteria(EvaluationMetric.ACCURACY, 0.4, "Precisi√≥n t√©cnica"),
    EvaluationCriteria(EvaluationMetric.CREATIVITY, 0.3, "Creatividad"),
    EvaluationCriteria(EvaluationMetric.SAFETY, 0.3, "Seguridad del contenido")
]
```

## üìö Recursos Adicionales

- [Documentaci√≥n de OpenAI](https://platform.openai.com/docs)
- [Paper original MetaGPT](https://arxiv.org/abs/2308.00352)
- [Gu√≠a de Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)
- [Mistral Prompt Optimization - Documento base](https://docs.mistral.ai/guides/prompting_capabilities/) - *T√©cnicas de optimizaci√≥n que inspiraron este framework*

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature
3. Haz commit de tus cambios
4. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver archivo LICENSE para detalles.

## ‚ö†Ô∏è Consideraciones Importantes

### üí∞ **Costos Estimados de API**
- **Prueba R√°pida**: $0.10-0.20 (recomendado para empezar)
- **Demo R√°pido**: $0.50-1.00
- **Demo Completo**: $2.00-4.00
- **Optimizaci√≥n Personalizada**: $1.00-3.00 (depende de iteraciones)

### ‚è±Ô∏è **Tiempos de Ejecuci√≥n**
- **Prueba R√°pida**: 2-3 minutos
- **Demo R√°pido**: 5-10 minutos  
- **Demo Completo**: 15-20 minutos
- **Comparaci√≥n de Estrategias**: 10-15 minutos

### üéØ **Consejos para Mejores Resultados**
- **Datos de Calidad**: Los resultados dependen de la calidad de los datos de entrada
- **Configuraci√≥n**: Ajusta los par√°metros seg√∫n tus necesidades espec√≠ficas
- **API Key**: Aseg√∫rate de tener cr√©ditos suficientes en tu cuenta OpenAI
- **Primeras Pruebas**: Usa `test_optimization.py` para validar tu configuraci√≥n

## üö® Soluci√≥n de Problemas Comunes

### ‚ùå "OPENAI_API_KEY NO configurado"
```bash
export OPENAI_API_KEY="tu-api-key-aqui"
```

### ‚ùå "ModuleNotFoundError"
```bash
source venv/bin/activate
pip install -r requirements.txt
```

### ‚ùå "Error de autenticaci√≥n OpenAI"
- Verifica que tu API key sea correcta
- Aseg√∫rate de tener cr√©ditos en tu cuenta OpenAI
- Revisa que no haya espacios extra en la API key

### ‚ùå "Timeout o errores de red"
- Reduce el n√∫mero de iteraciones: `--iterations 2`
- Usa modelos m√°s peque√±os en la configuraci√≥n
- Verifica tu conexi√≥n a internet

## üÜò Soporte

Para problemas o preguntas:
1. Revisa la documentaci√≥n
2. Ejecuta `python test_optimization.py` para diagn√≥stico
3. Prueba los ejemplos en `examples/` directory
4. Verifica los logs de error en la consola
5. Busca en issues existentes
6. Crea un nuevo issue con detalles espec√≠ficos

## üéâ Novedades de Esta Versi√≥n

### ‚ú® Soporte Completo para YAML

- **Entrada flexible**: Archivos YAML con prompts, tareas y datos integrados
- **Salida en YAML**: Resultados m√°s legibles y estructurados  
- **Detecci√≥n autom√°tica**: El sistema detecta el formato por extensi√≥n de archivo
- **Compatibilidad total**: Mantiene soporte completo para JSON

### üìÇ Directorio de Ejemplos Organizados

- **6 ejemplos probados**: Desde b√°sicos hasta avanzados
- **Casos de uso reales**: Clasificaci√≥n de trabajos, mensajes de chat
- **Formatos m√∫ltiples**: JSON y YAML demostrados
- **Resultados verificados**: Todos los ejemplos funcionan correctamente

### üîß Mejoras en la Interfaz

- **CLI m√°s flexible**: Argumentos opcionales cuando est√°n en YAML
- **Mejor organizaci√≥n**: Archivos separados por funci√≥n
- **Parser robusto**: Manejo de errores y formatos m√∫ltiples
- **Configuraci√≥n .env**: Carga autom√°tica de variables de entorno

## üéØ Pasos Recomendados para Empezar

1. **Configuraci√≥n inicial:**
   ```bash
   source venv/bin/activate
   export OPENAI_API_KEY="tu-api-key"
   ```

2. **Primera prueba:**
   ```bash
   python test_optimization.py
   ```

3. **Si funciona bien, prueba el demo:**
   ```bash
   python main.py job-title-example --mode quick
   ```

4. **Explora funcionalidades avanzadas seg√∫n tus necesidades**

---

**¬°Comienza a optimizar tus prompts hoy mismo!** üöÄ
# Self-Supervised Prompt Optimization (SPO) Framework

A complete implementation of the SPO framework based on MetaGPT for automated prompt optimization using OpenAI. This project is inspired by prompt optimization techniques documented by Mistral AI and adapted to work with OpenAI models.

## üöÄ Features

- **Iterative Optimization**: Automatic prompt improvement through multiple iterations
- **Multiple Strategies**: Support for different optimization approaches
- **Comprehensive Evaluation System**: Comprehensive metrics to evaluate prompt quality
- **Advanced Pipeline**: Strategy comparison and multi-objective optimization
- **MetaGPT Lambda Functions**: Faithful implementation of the original notebook with lambda functions
- **Confidence Scoring**: Advanced system to measure result reliability
- **Automatic Stopping Criteria**: Intelligent convergence detection
- **YAML/JSON Support**: Flexible formats for defining prompts and inputs
- **Practical Example**: Complete demonstration with job title classification

## üìÅ Project Structure

```
prompt_optimizer/
‚îú‚îÄ‚îÄ spo_framework.py          # Main SPO framework
‚îú‚îÄ‚îÄ optimization_pipeline.py  # Advanced pipeline with multiple strategies
‚îú‚îÄ‚îÄ evaluation_system.py      # Comprehensive evaluation system
‚îú‚îÄ‚îÄ lambda_functions.py       # MetaGPT-style lambda functions
‚îú‚îÄ‚îÄ confidence_scoring.py     # Confidence scoring system
‚îú‚îÄ‚îÄ yaml_parser.py            # Parser for YAML/JSON files
‚îú‚îÄ‚îÄ main.py                   # CLI interface
‚îú‚îÄ‚îÄ requirements.txt          # Dependencies (includes PyYAML)
‚îú‚îÄ‚îÄ .env                      # Environment variables (API keys)
‚îú‚îÄ‚îÄ .gitignore                # Files to ignore in git
‚îú‚îÄ‚îÄ examples/                 # Configuration examples (‚úÖ Tested)
‚îÇ   ‚îú‚îÄ‚îÄ sample_inputs.json        # Basic example in JSON
‚îÇ   ‚îú‚îÄ‚îÄ sample_inputs.yaml        # Basic example in YAML
‚îÇ   ‚îú‚îÄ‚îÄ sample_expected.json      # Expected outputs for examples
‚îÇ   ‚îú‚îÄ‚îÄ classification_example.yaml # Complete classification example
‚îÇ   ‚îî‚îÄ‚îÄ prompt_config_example.yaml  # Configuration with included prompt
‚îî‚îÄ‚îÄ README.md                # This file
```

## üõ†Ô∏è Installation

1. **Create Python 3.11 virtual environment**:
```bash
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Configure OpenAI API Key**:
```bash
export OPENAI_API_KEY="your-api-key-here"
```

## üéØ Quick Start

### üöÄ Quick Framework Test (Recommended to start)

```bash
# 1. Activate virtual environment
source venv/bin/activate

# 2. Configure OpenAI API key
export OPENAI_API_KEY="your-api-key-here"

# 3. Run quick test (2-3 minutes, ~$0.10-0.20)
python test_optimization.py
```

**What the quick test does:**
- ‚ö° 3 basic optimization iterations
- üéØ Use case: job title classification
- üìä Shows scores and improvements in real-time
- üí∞ Uses `gpt-4o-mini` for lower cost
- ‚è±Ô∏è Completes in 2-3 minutes

### Job Title Classification Example

```bash
# Quick demo (5-10 minutes, ~$0.50-1.00)
python main.py job-title-example --mode quick --output quick_results.json

# Full demo with all features (15-20 minutes, ~$2.00-4.00)
python main.py job-title-example --mode full --output full_results.json
```

### üéõÔ∏è Advanced Testing Options

#### Custom Optimization

```bash
# Using traditional JSON files
python main.py optimize \
  --prompt "Classify the job title into categories. Format: Category - Subcategory. Title: {job_title}" \
  --task "Job title classification" \
  --inputs examples/sample_inputs.json \
  --expected examples/sample_expected.json \
  --iterations 5 \
  --output optimization_results.json

# Using YAML files (new)
python main.py optimize \
  --prompt "Classify the job title into categories" \
  --task "Job title classification" \
  --inputs examples/sample_inputs.yaml \
  --expected examples/sample_expected.yaml \
  --iterations 5 \
  --output optimization_results.json

# Using complete YAML file with included prompt
python main.py optimize \
  --inputs examples/classification_example.yaml \
  --output optimization_results.json
```

#### Strategy Comparison
```bash
# With JSON files
python main.py compare \
  --prompt "Classify the job title into appropriate categories" \
  --task "Job title classification" \
  --inputs examples/sample_inputs.json \
  --strategies iterative_refinement ensemble_voting multi_objective \
  --output strategy_comparison.json

# With YAML files
python main.py compare \
  --inputs examples/classification_example.yaml \
  --strategies iterative_refinement ensemble_voting \
  --output strategy_comparison.json
```

#### Evaluation Only (no optimization)
```bash
# With JSON files
python main.py evaluate \
  --prompt "Your prompt to evaluate" \
  --inputs examples/sample_inputs.json \
  --outputs examples/sample_outputs.json \
  --expected examples/sample_expected.json \
  --metrics accuracy relevance clarity \
  --output evaluation_results.json

# With YAML files
python main.py evaluate \
  --prompt "Your prompt to evaluate" \
  --inputs examples/classification_example.yaml \
  --outputs examples/outputs.yaml \
  --expected examples/classification_example.yaml \
  --metrics accuracy relevance clarity \
  --output evaluation_results.json
```

## üìù YAML and JSON Format Support

### Complete YAML Format (Recommended)

The framework now supports YAML files that include the prompt, input data, and expected output all in one file:

```yaml
prompt: |
  You are an agent specialized in classifying user messages.
  Your task is to analyze each message and classify it into one of the following categories:
  - business_relevant: Messages related to business or product inquiries
  - support_request: Help requests or technical support
  - complaint: Complaints or expressions of dissatisfaction
  - compliment: Praise or positive comments
  
  Respond only with the corresponding category.

task: "Classify user messages into predefined categories"

data:
  - message: "Hello, do you have computers in stock?"
    classification: "business_relevant"
  - message: "My account is not working, I need help"
    classification: "support_request"
  - message: "I'm very upset because my order arrived late"
    classification: "complaint"
  - message: "Excellent service, highly recommended"
    classification: "compliment"

iterations: 5
model: "gpt-4o"
```

### Supported Formats

1. **YAML with combined data** (like the example above)
2. **YAML with separate sections**:
```yaml
inputs:
  - "Message 1"
  - "Message 2"
expected:
  - "Category 1"
  - "Category 2"
```

3. **Traditional JSON** (maintains compatibility):
```json
["input1", "input2", "input3"]
```

### YAML Parser Usage

```python
from yaml_parser import YAMLParser

# Load inputs from any format
inputs = YAMLParser.load_inputs_flexible("examples/file.yaml")

# Load expected outputs
expected = YAMLParser.load_expected_flexible("examples/file.yaml")

# Load complete configuration
config = YAMLParser.parse_prompt_config("examples/classification_example.yaml")
```

## üìÇ Examples Directory

The `examples/` directory contains example files for different use cases:

### üìÑ Available Files

1. **`sample_inputs.json`** - Basic example in JSON format
   ```json
   [
     "Senior Software Engineer at Google",
     "Data Scientist - Machine Learning",
     "Marketing Manager, Digital Products"
   ]
   ```

2. **`sample_inputs.yaml`** - Same content in YAML format
   ```yaml
   inputs:
     - "Senior Software Engineer at Google"
     - "Data Scientist - Machine Learning"
     - "Marketing Manager, Digital Products"
   ```

3. **`classification_example.yaml`** - Complete example with prompt and data
   ```yaml
   prompt: |
     Classify messages into specific categories...
   
   data:
     - message: "Hello, do you have stock?"
       classification: "business_relevant"
   ```

4. **`prompt_config_example.yaml`** - Configuration with multiple parameters
   ```yaml
   prompt: |
     Your prompt here...
   task: "Task description"
   iterations: 5
   model: "gpt-4o"
   strategies: ["iterative_refinement"]
   ```

### üöÄ How to Use the Examples (‚úÖ All Tested)

```bash
# 1. Basic example with external prompt
python main.py optimize \
  --inputs examples/sample_inputs.yaml \
  --prompt "Classify job titles into categories" \
  --task "Job title classification" \
  --iterations 2 \
  --output results.yaml

# 2. Complete example with included prompt
python main.py optimize \
  --inputs examples/classification_example.yaml \
  --iterations 3 \
  --output classification_results.yaml

# 3. Using traditional JSON files
python main.py optimize \
  --prompt "Classify these job titles" \
  --task "Job classification" \
  --inputs examples/sample_inputs.json \
  --expected examples/sample_expected.json \
  --output traditional_results.json
```

### ‚úÖ Test Results

The examples have been tested and work correctly:

- **`classification_example.yaml`**: ‚úÖ Perfect score (1.00) in 1 iteration
- **`prompt_config_example.yaml`**: ‚úÖ Perfect score (1.00) in 1 iteration
- **`sample_inputs.yaml`**: ‚úÖ Improved score (0.68) in 2 iterations  
- **YAML output**: ‚úÖ Readable and well-structured format
- **All formats**: ‚úÖ JSON and YAML working correctly

### YAML Format Advantages

- **Multiline prompts**: Use `|` for long and complex prompts
- **Single file**: Everything in one place (prompt, data, configuration)
- **More readable**: Clearer format than JSON
- **Comments**: You can add comments with `#`
- **Flexible**: Multiple formats supported
- **YAML output**: Results can also be saved in YAML

### üíæ Output Formats

The framework automatically detects the output format based on the file extension:

```bash
# JSON output (traditional)
python main.py optimize --inputs examples/classification_example.yaml --output results.json

# YAML output (new, more readable)
python main.py optimize --inputs examples/classification_example.yaml --output results.yaml
```

**YAML output example:**
```yaml
config:
  max_iterations: 5
  optimization_model: "gpt-4o"
  execution_model: "gpt-4o-mini"

optimization_history:
  - iteration: 1
    performance_score: 0.91
    optimized_prompt: "Optimized prompt..."
    execution_time: 45.2

summary:
  total_iterations: 5
  best_score: 0.95
  improvement: 0.15
  best_prompt: "Best prompt found..."
```

## üìä Optimization Strategies

### 1. Iterative Refinement (`iterative_refinement`)
- **Best for**: General-purpose optimization
- **Description**: Step-by-step incremental improvement
- **Advantages**: Fast and efficient

### 2. Ensemble Voting (`ensemble_voting`)
- **Best for**: Robust and reliable results
- **Description**: Multiple optimizations in parallel
- **Advantages**: Greater stability and accuracy

### 3. Multi-Objective (`multi_objective`)
- **Best for**: Balancing competing objectives
- **Description**: Optimizes for multiple criteria simultaneously
- **Advantages**: Balanced solutions

### 4. Genetic Algorithm (`genetic_algorithm`)
- **Best for**: Creative and diverse exploration
- **Description**: Evolution of prompts through generations
- **Advantages**: Finds innovative solutions

## üîß Programmatic Usage

### Basic Optimization

```python
import asyncio
from spo_framework import SPOFramework, PromptOptimizationConfig

async def optimize_prompt():
    config = PromptOptimizationConfig(
        max_iterations=5,
        optimization_model="gpt-4o",
        execution_model="gpt-4o-mini"
    )
    
    framework = SPOFramework(config, "your-api-key")
    
    result = await framework.optimize_prompt(
        initial_prompt="Your initial prompt",
        task_description="Task description",
        sample_inputs=["input1", "input2", "input3"],
        expected_outputs=["output1", "output2", "output3"]
    )
    
    print(f"Score: {result.performance_score}")
    print(f"Optimized prompt: {result.optimized_prompt}")

asyncio.run(optimize_prompt())
```

### MetaGPT-Style Optimization with Lambda Functions

```python
from lambda_functions import MetaGPTStyleOptimizer, PromptLambdaFactory

async def metagpt_optimization():
    # Create lambda function for job classification
    job_classifier = PromptLambdaFactory.create_job_classification_lambda()
    
    # MetaGPT-style optimizer
    optimizer = MetaGPTStyleOptimizer(client, config)
    
    result = await optimizer.optimize_with_lambda(
        initial_lambda=job_classifier,
        task_description="Classify job titles",
        test_inputs=job_titles,
        expected_outputs=expected_classifications,
        max_rounds=5
    )
```

### Advanced Pipeline

```python
from optimization_pipeline import AdvancedOptimizationPipeline, PipelineConfig, OptimizationStrategy

async def advanced_optimization():
    pipeline_config = PipelineConfig(
        strategy=OptimizationStrategy.ENSEMBLE_VOTING,
        ensemble_size=5
    )
    
    pipeline = AdvancedOptimizationPipeline(spo_config, pipeline_config, api_key)
    
    result = await pipeline.optimize_with_strategy(
        initial_prompt="Your prompt",
        task_description="Task",
        sample_inputs=inputs,
        expected_outputs=outputs
    )
```

### Confidence System

```python
from confidence_scoring import ConfidenceAnalyzer

async def analyze_confidence():
    analyzer = ConfidenceAnalyzer(client)
    
    confidence_metrics = await analyzer.analyze_optimization_confidence(
        optimization_history=optimization_results,
        test_inputs=test_data,
        final_prompt=best_prompt,
        num_validation_runs=5
    )
    
    report = analyzer.generate_confidence_report(confidence_metrics)
    print(f"Confidence Level: {report['confidence_level']}")
    print(f"Reliability Index: {report['reliability_index']:.2f}")
```

### Comprehensive Evaluation

```python
from evaluation_system import ComprehensiveEvaluationSystem, EvaluationCriteria, EvaluationMetric

async def evaluate_prompt():
    evaluation_system = ComprehensiveEvaluationSystem(api_key)
    
    criteria = [
        EvaluationCriteria(EvaluationMetric.ACCURACY, 0.5, "Accuracy"),
        EvaluationCriteria(EvaluationMetric.CLARITY, 0.3, "Clarity"),
        EvaluationCriteria(EvaluationMetric.RELEVANCE, 0.2, "Relevance")
    ]
    
    result = await evaluation_system.comprehensive_evaluate(
        prompt="Your prompt",
        input_text="Test input",
        output="Generated output",
        expected="Expected output",
        evaluation_criteria=criteria
    )
```

## üìà Evaluation Metrics

- **Accuracy**: Precision and correctness of output
- **Relevance**: Relevance to input and task
- **Clarity**: Clarity and readability
- **Completeness**: Completeness of response
- **Consistency**: Consistency across multiple executions
- **Efficiency**: Prompt efficiency
- **Adherence to Format**: Adherence to specified format
- **Confidence Metrics**: Reliability and stability metrics

## üîç Monitoring and Results

Results can be exported in JSON format for later analysis:

```json
{
  "config": {...},
  "optimization_history": [...],
  "summary": {
    "total_iterations": 5,
    "best_score": 0.89,
    "improvement": 0.34,
    "best_prompt": "Final optimized prompt"
  }
}
```

## üéØ Detailed Example: Job Title Classification

The framework includes a complete example that demonstrates optimization of a job title classification system:

```python
from job_title_example import JobTitleClassificationExample

async def run_example():
    example = JobTitleClassificationExample(api_key)
    results = await example.run_complete_example()
    
    # Runs:
    # 1. Basic SPO optimization
    # 2. Strategy comparison
    # 3. Comprehensive evaluation
    # 4. Final report with recommendations
```

## üöÄ Use Cases

- **Chatbot Improvement**: Optimize prompts for more natural conversations
- **Text Classification**: Improve accuracy in categorization tasks
- **Content Generation**: Optimize prompts for quality content
- **Sentiment Analysis**: Fine-tune prompts for better emotional detection
- **Information Extraction**: Improve prompts for extracting structured data

## üîß Advanced Configuration

### Custom Models

```python
config = PromptOptimizationConfig(
    optimization_model="gpt-4o",        # Model for optimization
    execution_model="gpt-4o-mini",      # Model for execution
    evaluation_model="gpt-4o",          # Model for evaluation
    temperature=0.7,                     # Creativity
    max_tokens=2000                      # Token limit
)
```

### Custom Evaluation Criteria

```python
custom_criteria = [
    EvaluationCriteria(EvaluationMetric.ACCURACY, 0.4, "Technical precision"),
    EvaluationCriteria(EvaluationMetric.CREATIVITY, 0.3, "Creativity"),
    EvaluationCriteria(EvaluationMetric.SAFETY, 0.3, "Content safety")
]
```

## üìö Additional Resources

- [OpenAI Documentation](https://platform.openai.com/docs)
- [Original MetaGPT Paper](https://arxiv.org/abs/2308.00352)
- [Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)
- [Mistral Prompt Optimization - Base Document](https://docs.mistral.ai/guides/prompting_capabilities/) - *Optimization techniques that inspired this framework*

## ü§ù Contributions

Contributions are welcome. Please:

1. Fork the repository
2. Create a branch for your feature
3. Commit your changes
4. Open a Pull Request

## üìÑ License

This project is under the MIT license. See LICENSE file for details.

## ‚ö†Ô∏è Important Considerations

### üí∞ **Estimated API Costs**
- **Quick Test**: $0.10-0.20 (recommended to start)
- **Quick Demo**: $0.50-1.00
- **Full Demo**: $2.00-4.00
- **Custom Optimization**: $1.00-3.00 (depends on iterations)

### ‚è±Ô∏è **Execution Times**
- **Quick Test**: 2-3 minutes
- **Quick Demo**: 5-10 minutes  
- **Full Demo**: 15-20 minutes
- **Strategy Comparison**: 10-15 minutes

### üéØ **Tips for Better Results**
- **Quality Data**: Results depend on the quality of input data
- **Configuration**: Adjust parameters according to your specific needs
- **API Key**: Make sure you have sufficient credits in your OpenAI account
- **First Tests**: Use `test_optimization.py` to validate your configuration

## üö® Common Troubleshooting

### ‚ùå "OPENAI_API_KEY NOT configured"
```bash
export OPENAI_API_KEY="your-api-key-here"
```

### ‚ùå "ModuleNotFoundError"
```bash
source venv/bin/activate
pip install -r requirements.txt
```

### ‚ùå "OpenAI authentication error"
- Verify that your API key is correct
- Make sure you have credits in your OpenAI account
- Check that there are no extra spaces in the API key

### ‚ùå "Timeout or network errors"
- Reduce the number of iterations: `--iterations 2`
- Use smaller models in configuration
- Verify your internet connection

## üÜò Support

For problems or questions:
1. Review the documentation
2. Run `python test_optimization.py` for diagnosis
3. Try the examples in `examples/` directory
4. Check error logs in the console
5. Search in existing issues
6. Create a new issue with specific details

## üéâ What's New in This Version

### ‚ú® Complete YAML Support

- **Flexible input**: YAML files with integrated prompts, tasks, and data
- **YAML output**: More readable and structured results  
- **Automatic detection**: System detects format by file extension
- **Full compatibility**: Maintains complete JSON support

### üìÇ Organized Examples Directory

- **5 tested examples**: From basic to advanced
- **Real use cases**: Job classification, message classification
- **Multiple formats**: JSON and YAML demonstrated
- **Verified results**: All examples work correctly

### üîß Interface Improvements

- **More flexible CLI**: Optional arguments when in YAML
- **Better organization**: Files separated by function
- **Robust parser**: Error handling and multiple formats
- **Environment configuration**: Automatic loading of environment variables

## üéØ Recommended Steps to Get Started

1. **Initial configuration:**
   ```bash
   source venv/bin/activate
   export OPENAI_API_KEY="your-api-key"
   ```

2. **First test:**
   ```bash
   python test_optimization.py
   ```

3. **If it works well, try the demo:**
   ```bash
   python main.py job-title-example --mode quick
   ```

4. **Explore advanced features according to your needs**

---

**Start optimizing your prompts today!** üöÄ